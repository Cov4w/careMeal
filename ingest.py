import os
import shutil
from dotenv import load_dotenv
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader, TextLoader, CSVLoader, UnstructuredExcelLoader, BSHTMLLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def ingest_data():
    persist_directory = "./chroma_db"
    data_directory = "./data"

    print("ğŸš€ [Ingest] ë°ì´í„° í•™ìŠµ ë° ë²¡í„° DB ìƒì„± ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # 0. ê¸°ì¡´ DB ì‚­ì œ (Warning: ì™„ì „ ì¬í•™ìŠµ)
    if os.path.exists(persist_directory):
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ DB({persist_directory})ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        shutil.rmtree(persist_directory)

    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë¡œì»¬ CPU)
    print("Start Loading Embeddings Model...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    if not os.path.exists(data_directory):
        print(f"âš ï¸ '{data_directory}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. í´ë”ë¥¼ ìƒì„±í•˜ê³  ë¬¸ì„œë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
        os.makedirs(data_directory)
        return

    documents = []
    
    # 2. ë¬¸ì„œ ë¡œë” ì„¤ì •
    loaders = [
        ("PDF", DirectoryLoader(data_directory, glob="**/*.pdf", loader_cls=PyMuPDFLoader, silent_errors=True)),
        ("Text", DirectoryLoader(data_directory, glob="**/*.txt", loader_cls=TextLoader, silent_errors=True)),
        ("CSV", DirectoryLoader(data_directory, glob="**/*.csv", loader_cls=CSVLoader, silent_errors=True)),
        ("Excel", DirectoryLoader(data_directory, glob="**/*.xlsx", loader_cls=UnstructuredExcelLoader, silent_errors=True)),
        ("HTML", DirectoryLoader(data_directory, glob="**/*.html", loader_cls=BSHTMLLoader, silent_errors=True)),
    ]
    
    for name, loader in loaders:
        try:
            print(f"ğŸ“š Reading {name} files...")
            docs = loader.load()
            print(f"  - {len(docs)} {name} documents loaded.")
            documents.extend(docs)
        except Exception as e:
            print(f"âš ï¸ {name} ë¡œë”© ì¤‘ ì—ëŸ¬ (ê±´ë„ˆëœ€): {e}")

    # 3. ì²­í‚¹ ë° ì €ì¥
    if documents:
        print(f"ğŸ“„ ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ì²­í‚¹ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
        splits = text_splitter.split_documents(documents)
        
        print(f"ğŸ§© {len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„° DB ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        vector_store = Chroma.from_documents(
            documents=splits, 
            embedding=embeddings, 
            persist_directory=persist_directory
        )
        print(f"ğŸ’¾ ë²¡í„° DB ì €ì¥ ì™„ë£Œ! ({persist_directory})")
        print("âœ… ì´ì œ 'python main.py' (uvicorn)ë¥¼ ì‹¤í–‰í•˜ë©´ ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. data í´ë”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    ingest_data()
